# Korean RAG Chatbot - ChromaDB + Ollama
import asyncio
import base64
import gc
import hashlib
import os
import re
import uuid
from pathlib import Path

import chromadb
import ollama
from dotenv import load_dotenv
from pypdf import PdfReader
import streamlit as st

from etl.embedding_generation import OllamaEmbedding
from _party import obsidian
from _party.jira_client import JiraClient, JiraConfig, extract_issues_content
from _party.notion_client import NotionClient, NotionConfig, extract_pages_content

# Load environment variables
load_dotenv()

# Configuration from .env
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_LLM_MODEL = os.getenv("OLLAMA_LLM_MODEL", "gemma3:1b")
OLLAMA_EMBEDDING_MODEL = os.getenv("OLLAMA_EMBEDDING_MODEL", "bge-m3")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "500"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "100"))
TOP_K_RESULTS = int(os.getenv("TOP_K_RESULTS", "5"))

# Obsidian default path
DEFAULT_OBSIDIAN_PATH = os.getenv("OBSIDIAN_VAULT_PATH", "")

# Jira defaults
DEFAULT_JIRA_URL = os.getenv("JIRA_URL", "")
DEFAULT_JIRA_EMAIL = os.getenv("JIRA_EMAIL", "")
DEFAULT_JIRA_TOKEN = os.getenv("JIRA_API_TOKEN", "")

# Notion defaults
DEFAULT_NOTION_TOKEN = os.getenv("NOTION_API_TOKEN", "")

# Initialize clients
chroma_client = chromadb.Client()
embedding_generator = OllamaEmbedding()


# ============================================================================
# Core Functions
# ============================================================================

def get_embeddings(texts: list[str]) -> list[list[float]]:
    """Generate embeddings for a list of texts."""
    async def _generate():
        return await embedding_generator.generate_embeddings_batch(texts)
    return asyncio.run(_generate())


def clean_korean_text(text: str) -> str:
    """Clean and normalize Korean text."""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'^\d+\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'- \d+ -', '', text)
    text = text.replace('ï¼', '.').replace('ï¼Œ', ',')
    return text.strip()


def chunk_korean_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> list[str]:
    """Chunk Korean text with awareness of sentence boundaries."""
    text = clean_korean_text(text)

    sentence_endings = re.compile(r'([.!?ã€‚]\s*|ë‹¤\.\s*|ìš”\.\s*|ë‹ˆë‹¤\.\s*|ìŠµë‹ˆë‹¤\.\s*)')
    sentences = sentence_endings.split(text)

    full_sentences = []
    i = 0
    while i < len(sentences):
        sentence = sentences[i]
        if i + 1 < len(sentences) and sentence_endings.match(sentences[i + 1]):
            sentence += sentences[i + 1]
            i += 2
        else:
            i += 1
        if sentence.strip():
            full_sentences.append(sentence.strip())

    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in full_sentences:
        sentence_len = len(sentence)

        if current_length + sentence_len > chunk_size and current_chunk:
            chunks.append(' '.join(current_chunk))
            overlap_text = ' '.join(current_chunk)
            while len(overlap_text) > overlap and current_chunk:
                current_chunk.pop(0)
                overlap_text = ' '.join(current_chunk)
            current_length = len(overlap_text)

        current_chunk.append(sentence)
        current_length += sentence_len

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return [c for c in chunks if len(c) > 50]


def generate_collection_name(name: str, session_id: str) -> str:
    """Generate a valid ChromaDB collection name."""
    file_hash = hashlib.md5(name.encode()).hexdigest()[:8]
    return f"rag-{session_id[:8]}-{file_hash}"


# ============================================================================
# PDF Functions
# ============================================================================

def extract_text_from_pdf(file) -> str:
    """Extract text from PDF file."""
    reader = PdfReader(file)
    pages = []
    for page in reader.pages:
        text = page.extract_text()
        if text:
            pages.append(text)
    return "\n".join(pages)


# ============================================================================
# Indexing Functions
# ============================================================================

def index_documents(chunks: list[str], metadata: list[dict], collection_name: str):
    """Index chunks with embeddings into ChromaDB."""
    if not chunks:
        return None, 0

    collection = chroma_client.get_or_create_collection(
        name=collection_name,
        metadata={"hnsw:space": "cosine"}
    )

    embeddings = get_embeddings(chunks)

    collection.add(
        ids=[f"chunk_{i}" for i in range(len(chunks))],
        embeddings=embeddings,
        documents=chunks,
        metadatas=metadata if metadata else None
    )

    return collection, len(chunks)


# ============================================================================
# LLM Query
# ============================================================================

def query_llm(prompt: str, context: str):
    """Query LLM with RAG context."""
    system_prompt = f"""ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}"""

    return ollama.chat(
        model=OLLAMA_LLM_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        stream=True
    )


# ============================================================================
# UI Functions
# ============================================================================

def reset_chat():
    """Reset chat state."""
    st.session_state.messages = []
    st.session_state.collection = None
    gc.collect()


def display_pdf(file):
    """Display PDF preview."""
    file.seek(0)
    base64_pdf = base64.b64encode(file.read()).decode("utf-8")
    st.markdown(
        f'<iframe src="data:application/pdf;base64,{base64_pdf}" '
        f'width="100%" height="400" type="application/pdf"></iframe>',
        unsafe_allow_html=True
    )


# ============================================================================
# Main App
# ============================================================================

# Initialize session state (each key separately to handle upgrades)
if "id" not in st.session_state:
    st.session_state.id = str(uuid.uuid4())
if "messages" not in st.session_state:
    st.session_state.messages = []
if "collection" not in st.session_state:
    st.session_state.collection = None
if "obsidian_path" not in st.session_state:
    st.session_state.obsidian_path = DEFAULT_OBSIDIAN_PATH
if "source_type" not in st.session_state:
    st.session_state.source_type = None
if "jira_config" not in st.session_state:
    st.session_state.jira_config = None
    # Auto-configure Jira if credentials in .env
    if DEFAULT_JIRA_URL and DEFAULT_JIRA_EMAIL and DEFAULT_JIRA_TOKEN:
        st.session_state.jira_config = JiraConfig(
            url=DEFAULT_JIRA_URL,
            email=DEFAULT_JIRA_EMAIL,
            api_token=DEFAULT_JIRA_TOKEN
        )
if "jira_issues" not in st.session_state:
    st.session_state.jira_issues = []
if "notion_config" not in st.session_state:
    st.session_state.notion_config = None
    # Auto-configure Notion if token in .env
    if DEFAULT_NOTION_TOKEN:
        st.session_state.notion_config = NotionConfig(api_token=DEFAULT_NOTION_TOKEN)
if "notion_pages" not in st.session_state:
    st.session_state.notion_pages = []

session_id = st.session_state.id

# Page config
st.set_page_config(page_title="Korean RAG", layout="wide")

# Sidebar
with st.sidebar:
    st.header("ğŸ“š ì†ŒìŠ¤ ì„ íƒ")

    source_type = st.radio(
        "ë°ì´í„° ì†ŒìŠ¤",
        ["PDF ì—…ë¡œë“œ", "Obsidian ë³¼íŠ¸", "Jira", "Notion"],
        key="source_selector"
    )

    st.divider()

    # ========== PDF Upload ==========
    if source_type == "PDF ì—…ë¡œë“œ":
        st.subheader("ğŸ“„ PDF ë¬¸ì„œ")
        uploaded_file = st.file_uploader("PDF íŒŒì¼ ì„ íƒ", type="pdf")

        if uploaded_file:
            try:
                collection_name = generate_collection_name(uploaded_file.name, session_id)

                if st.session_state.collection is None or st.session_state.source_type != "pdf":
                    with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
                        uploaded_file.seek(0)
                        text = extract_text_from_pdf(uploaded_file)
                        chunks = chunk_korean_text(text)

                        if not chunks:
                            st.error("í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            st.stop()

                        metadata = [{"source": uploaded_file.name} for _ in chunks]
                        collection, chunk_count = index_documents(chunks, metadata, collection_name)

                        st.session_state.collection = collection
                        st.session_state.source_type = "pdf"
                        st.success(f"âœ… ì¤€ë¹„ ì™„ë£Œ! ({chunk_count}ê°œ ì²­í¬)")

                st.markdown("### ë¯¸ë¦¬ë³´ê¸°")
                uploaded_file.seek(0)
                display_pdf(uploaded_file)

            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")

    # ========== Obsidian Vault ==========
    elif source_type == "Obsidian ë³¼íŠ¸":
        st.subheader("ğŸ—ƒï¸ Obsidian ë³¼íŠ¸")

        vault_path = st.text_input(
            "ë³¼íŠ¸ ê²½ë¡œ",
            value=st.session_state.obsidian_path,
            placeholder="C:/Users/username/Documents/Obsidian Vault"
        )

        if vault_path:
            st.session_state.obsidian_path = vault_path
            vault = Path(vault_path)

            if vault.exists():
                # Get folders using _party.obsidian
                folders = obsidian.get_vault_folders(vault)
                folder_names = [str(f.relative_to(vault)) if f != vault else "ğŸ“ ì „ì²´ ë³¼íŠ¸" for f in folders]

                selected_folder_name = st.selectbox("í´ë” ì„ íƒ", folder_names)

                if selected_folder_name == "ğŸ“ ì „ì²´ ë³¼íŠ¸":
                    selected_folder = vault
                else:
                    selected_folder = vault / selected_folder_name

                # Show files in folder
                md_files = obsidian.get_markdown_files(selected_folder, recursive=False)

                if md_files:
                    st.markdown(f"**íŒŒì¼ ({len(md_files)}ê°œ)**")
                    for f in md_files[:10]:
                        st.text(f"  ğŸ“ {f.name}")
                    if len(md_files) > 10:
                        st.text(f"  ... ì™¸ {len(md_files) - 10}ê°œ")

                # Options
                recursive = st.checkbox("í•˜ìœ„ í´ë” í¬í•¨", value=True)

                # Index button
                if st.button("ğŸ” ì¸ë±ì‹± ì‹œì‘", type="primary"):
                    with st.spinner("ì¸ë±ì‹± ì¤‘..."):
                        files = obsidian.get_markdown_files(selected_folder, recursive)

                        if not files:
                            st.warning("ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            # Extract content using _party.obsidian
                            chunks, metadata = obsidian.extract_vault_content(
                                files, vault, chunk_korean_text
                            )

                            if chunks:
                                collection_name = generate_collection_name(selected_folder_name, session_id)
                                collection, chunk_count = index_documents(chunks, metadata, collection_name)

                                st.session_state.collection = collection
                                st.session_state.source_type = "obsidian"
                                st.success(f"âœ… ì™„ë£Œ! {len(files)}ê°œ íŒŒì¼, {chunk_count}ê°œ ì²­í¬")
                            else:
                                st.error("ì¸ë±ì‹± ì‹¤íŒ¨: í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.warning("ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ========== Jira ==========
    elif source_type == "Jira":
        st.subheader("ğŸ« Jira")

        # Connection settings
        with st.expander("ì—°ê²° ì„¤ì •", expanded=st.session_state.jira_config is None):
            jira_url = st.text_input(
                "Jira URL",
                value=DEFAULT_JIRA_URL,
                placeholder="https://your-domain.atlassian.net"
            )
            jira_email = st.text_input(
                "ì´ë©”ì¼ / ì‚¬ìš©ìëª…",
                value=DEFAULT_JIRA_EMAIL,
                placeholder="user@example.com"
            )
            jira_token = st.text_input(
                "API í† í°",
                value=DEFAULT_JIRA_TOKEN,
                type="password",
                placeholder="API token or password"
            )

            if st.button("ğŸ”— ì—°ê²° í…ŒìŠ¤íŠ¸"):
                if jira_url and jira_email and jira_token:
                    config = JiraConfig(url=jira_url, email=jira_email, api_token=jira_token)
                    client = JiraClient(config)
                    success, message = client.test_connection()

                    if success:
                        st.success(message)
                        st.session_state.jira_config = config
                    else:
                        st.error(message)
                else:
                    st.warning("ëª¨ë“  í•„ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        # If connected, show project/search options
        if st.session_state.jira_config:
            client = JiraClient(st.session_state.jira_config)

            # Get projects
            projects = client.get_projects()

            if projects:
                project_options = ["ì§ì ‘ JQL ì…ë ¥"] + [f"{p['key']} - {p['name']}" for p in projects]
                selected_project = st.selectbox("í”„ë¡œì íŠ¸ ì„ íƒ", project_options)

                if selected_project == "ì§ì ‘ JQL ì…ë ¥":
                    jql = st.text_input(
                        "JQL ì¿¼ë¦¬",
                        placeholder="project = MYPROJ AND status = Open"
                    )
                else:
                    project_key = selected_project.split(" - ")[0]
                    jql = f"project = {project_key} ORDER BY updated DESC"
                    st.code(jql, language="sql")

                max_issues = st.slider("ìµœëŒ€ ì´ìŠˆ ìˆ˜", 10, 100, 50)

                if st.button("ğŸ” ì´ìŠˆ ê°€ì ¸ì˜¤ê¸°", type="primary"):
                    if jql:
                        with st.spinner("ì´ìŠˆ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                            issues = client.search_issues(jql, max_results=max_issues)

                            if issues:
                                st.info(f"ğŸ“‹ {len(issues)}ê°œ ì´ìŠˆ ë°œê²¬")

                                # Store issues for display
                                st.session_state.jira_issues = issues

                                # Extract and index content
                                chunks, metadata = extract_issues_content(
                                    issues, chunk_korean_text
                                )

                                if chunks:
                                    collection_name = generate_collection_name(
                                        f"jira-{jql[:20]}", session_id
                                    )
                                    collection, chunk_count = index_documents(
                                        chunks, metadata, collection_name
                                    )

                                    st.session_state.collection = collection
                                    st.session_state.source_type = "jira"
                                    st.success(f"âœ… ì™„ë£Œ! {len(issues)}ê°œ ì´ìŠˆ, {chunk_count}ê°œ ì²­í¬")
                                else:
                                    st.warning("ì´ìŠˆì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            else:
                                if client.last_error:
                                    st.error(f"Jira API ì˜¤ë¥˜: {client.last_error}")
                                else:
                                    st.warning(f"ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. JQL: {jql}")
                    else:
                        st.warning("JQL ì¿¼ë¦¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

                # Display fetched issues
                if st.session_state.jira_issues:
                    with st.expander(f"ğŸ“‹ ì´ìŠˆ ëª©ë¡ ({len(st.session_state.jira_issues)}ê°œ)", expanded=False):
                        for issue in st.session_state.jira_issues:
                            key = issue.get("key", "")
                            fields = issue.get("fields", {})
                            summary = fields.get("summary", "ì œëª© ì—†ìŒ")
                            status = fields.get("status", {}).get("name", "")
                            st.markdown(f"**{key}**: {summary} `{status}`")
            else:
                if client.last_error:
                    st.error(f"í”„ë¡œì íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {client.last_error}")
                else:
                    st.warning("í”„ë¡œì íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    # ========== Notion ==========
    else:  # Notion
        st.subheader("ğŸ“ Notion")

        # Connection settings
        with st.expander("ì—°ê²° ì„¤ì •", expanded=st.session_state.notion_config is None):
            notion_token = st.text_input(
                "Integration Token",
                value=DEFAULT_NOTION_TOKEN,
                type="password",
                placeholder="secret_xxx..."
            )
            st.caption("ğŸ”— [Notion Integrations](https://www.notion.so/my-integrations)ì—ì„œ í† í°ì„ ìƒì„±í•˜ì„¸ìš”")

            if st.button("ğŸ”— ì—°ê²° í…ŒìŠ¤íŠ¸", key="notion_test"):
                if notion_token:
                    config = NotionConfig(api_token=notion_token)
                    client = NotionClient(config)
                    success, message = client.test_connection()

                    if success:
                        st.success(message)
                        st.session_state.notion_config = config
                    else:
                        st.error(message)
                else:
                    st.warning("Integration Tokenì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        # If connected, show search options
        if st.session_state.notion_config:
            client = NotionClient(st.session_state.notion_config)

            search_query = st.text_input(
                "ê²€ìƒ‰ì–´ (ì„ íƒ)",
                placeholder="ê²€ìƒ‰ì–´ ì…ë ¥ ë˜ëŠ” ë¹„ì›Œë‘ë©´ ì „ì²´ ê²€ìƒ‰"
            )

            filter_options = ["ì „ì²´", "í˜ì´ì§€ë§Œ", "ë°ì´í„°ë² ì´ìŠ¤ë§Œ"]
            filter_choice = st.selectbox("í•„í„°", filter_options)

            filter_type = None
            if filter_choice == "í˜ì´ì§€ë§Œ":
                filter_type = "page"
            elif filter_choice == "ë°ì´í„°ë² ì´ìŠ¤ë§Œ":
                filter_type = "database"

            max_pages = st.slider("ìµœëŒ€ í˜ì´ì§€ ìˆ˜", 10, 100, 50, key="notion_max")

            if st.button("ğŸ” í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°", type="primary", key="notion_fetch"):
                with st.spinner("í˜ì´ì§€ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                    results = client.search(
                        query=search_query,
                        filter_type=filter_type,
                        page_size=max_pages
                    )

                    # Filter to only pages (not databases)
                    pages = [r for r in results if r.get("object") == "page"]

                    if pages:
                        st.info(f"ğŸ“‹ {len(pages)}ê°œ í˜ì´ì§€ ë°œê²¬")

                        # Store pages for display
                        st.session_state.notion_pages = pages

                        # Extract and index content
                        chunks, metadata = extract_pages_content(
                            client, pages, chunk_korean_text
                        )

                        if chunks:
                            collection_name = generate_collection_name(
                                f"notion-{search_query[:20] if search_query else 'all'}", session_id
                            )
                            collection, chunk_count = index_documents(
                                chunks, metadata, collection_name
                            )

                            st.session_state.collection = collection
                            st.session_state.source_type = "notion"
                            st.success(f"âœ… ì™„ë£Œ! {len(pages)}ê°œ í˜ì´ì§€, {chunk_count}ê°œ ì²­í¬")
                        else:
                            st.warning("í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        if client.last_error:
                            st.error(f"Notion API ì˜¤ë¥˜: {client.last_error}")
                        else:
                            st.warning("í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # Display fetched pages
            if st.session_state.notion_pages:
                with st.expander(f"ğŸ“‹ í˜ì´ì§€ ëª©ë¡ ({len(st.session_state.notion_pages)}ê°œ)", expanded=False):
                    for page in st.session_state.notion_pages:
                        from _party.notion_client import extract_page_title
                        title = extract_page_title(page)
                        url = page.get("url", "")
                        st.markdown(f"**{title}** [ğŸ”—]({url})" if url else f"**{title}**")

    st.divider()

    # Status
    if st.session_state.collection:
        st.success("ğŸ’¡ ì§ˆë¬¸í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        st.info("ğŸ“¤ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë³¼íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”.")

# Main area
col1, col2 = st.columns([6, 1])
with col1:
    st.header("ğŸ’¬ ë¬¸ì„œ Q&A")
with col2:
    st.button("ì´ˆê¸°í™”", on_click=reset_chat)

# Chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    if st.session_state.collection is None:
        st.warning("ë¨¼ì € ë¬¸ì„œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Special handling for status-based Jira queries
    status_context = ""
    if st.session_state.source_type == "jira" and st.session_state.jira_issues:
        prompt_lower = prompt.lower()
        # Check for status keywords
        done_keywords = ["done", "ì™„ë£Œ", "closed", "resolved", "finished", "ëë‚œ"]
        progress_keywords = ["progress", "ì§„í–‰", "working", "in progress", "ì§„í–‰ì¤‘"]
        pending_keywords = ["pending", "ëŒ€ê¸°", "todo", "to do", "í• ì¼", "open", "backlog"]

        filtered_issues = []
        filter_type = None

        if any(kw in prompt_lower for kw in done_keywords):
            filter_type = "ì™„ë£Œ"
            for issue in st.session_state.jira_issues:
                status = issue.get("fields", {}).get("status", {})
                status_name = status.get("name", "").lower()
                status_cat = status.get("statusCategory", {}).get("name", "")
                if status_cat == "Done" or any(kw in status_name for kw in ["done", "ì™„ë£Œ", "closed", "resolved"]):
                    filtered_issues.append(issue)
        elif any(kw in prompt_lower for kw in progress_keywords):
            filter_type = "ì§„í–‰ì¤‘"
            for issue in st.session_state.jira_issues:
                status = issue.get("fields", {}).get("status", {})
                status_name = status.get("name", "").lower()
                status_cat = status.get("statusCategory", {}).get("name", "")
                if status_cat == "In Progress" or any(kw in status_name for kw in ["progress", "ì§„í–‰"]):
                    filtered_issues.append(issue)
        elif any(kw in prompt_lower for kw in pending_keywords):
            filter_type = "ëŒ€ê¸°"
            for issue in st.session_state.jira_issues:
                status = issue.get("fields", {}).get("status", {})
                status_name = status.get("name", "").lower()
                status_cat = status.get("statusCategory", {}).get("name", "")
                if status_cat == "To Do" or any(kw in status_name for kw in ["todo", "open", "backlog", "ëŒ€ê¸°"]):
                    filtered_issues.append(issue)

        if filtered_issues:
            status_context = f"\n\n[{filter_type} ìƒíƒœ ì´ìŠˆ {len(filtered_issues)}ê°œ]\n"
            for issue in filtered_issues:
                key = issue.get("key", "")
                fields = issue.get("fields", {})
                summary = fields.get("summary", "")
                status_name = fields.get("status", {}).get("name", "")
                status_context += f"- {key}: {summary} (ìƒíƒœ: {status_name})\n"

    # Search - increase results for "all/ì „ì²´/ëª©ë¡" queries
    all_keywords = ["all", "ëª¨ë“ ", "ì „ì²´", "ëª©ë¡", "ë¦¬ìŠ¤íŠ¸", "list", "show me", "ë³´ì—¬"]
    n_results = TOP_K_RESULTS
    if any(kw in prompt.lower() for kw in all_keywords):
        n_results = min(30, st.session_state.collection.count())  # Get more results

    query_embedding = get_embeddings([prompt])[0]
    results = st.session_state.collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results,
        include=["documents", "metadatas"]
    )

    # Build context with source info
    context_parts = []
    if status_context:
        context_parts.append(status_context)  # Add filtered status info first
    if results["documents"]:
        for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
            source = meta.get("source", "unknown") if meta else "unknown"
            context_parts.append(f"[ì¶œì²˜: {source}]\n{doc}")

    context = "\n\n---\n\n".join(context_parts)

    # Generate response
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""

        for chunk in query_llm(prompt, context):
            delta = chunk["message"]["content"]
            full_response += delta
            placeholder.markdown(full_response + "â–Œ")

        placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
